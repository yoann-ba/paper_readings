“A Simple Framework for Contrastive Learning of Visual Representations”
https://arxiv.org/abs/2002.05709

> “We simplify recently proposed contrastive selfsupervised learning algorithms without requiring specialized architectures or a memory bank.”

> “We show that 

(1) composition of data augmentations plays a critical role in defining effective predictive tasks, 
(2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and 
(3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning.”
> “A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy”



Article https://medium.com/analytics-vidhya/contrastive-learning-in-computer-vision-brief-concept-overview-and-sample-code-787c9038e809

Paper	https://arxiv.org/abs/2002.05709

PwC summary https://paperswithcode.com/method/simclr#:~:text=SimCLR%20is%20a%20framework%20for,loss%20in%20the%20latent%20space.

Details the working of SIMCLR.